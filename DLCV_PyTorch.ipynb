{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "DLCV_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "t6be-ndfYg8H"
      ]
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "81944e45b6424f9a8dc0bfba703584ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0495658e14a047b0931a573a863cd89c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5f88d6751c47422f926d35108ad654b1",
              "IPY_MODEL_76143aff8cd64c318903177fef9eea85"
            ]
          }
        },
        "0495658e14a047b0931a573a863cd89c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f88d6751c47422f926d35108ad654b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_53c36d21e6524b46a77d486528452a0f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2f7ba9894fda4341857d3bbedb77e330"
          }
        },
        "76143aff8cd64c318903177fef9eea85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_207701d378e34dfe857f90e513b8d6e0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 44.7M/44.7M [00:00&lt;00:00, 80.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e3d2d503ea2423f9f30f249789d537d"
          }
        },
        "53c36d21e6524b46a77d486528452a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2f7ba9894fda4341857d3bbedb77e330": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "207701d378e34dfe857f90e513b8d6e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e3d2d503ea2423f9f30f249789d537d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HQJ1nfJ1hB4",
        "colab_type": "text"
      },
      "source": [
        "# $\\text{DLCV: PyTorch Tutorial}$\n",
        "\n",
        "$\\text{Prepared by:}$ [$\\text{Naveen Venkat}$](http://naveenvenkat.com)$^1$, $\\text{Gaurang Sriramanan}$$^2$.\n",
        "\n",
        "$\\text{Video Analytics Lab, Indian Institute of Science, Bangalore.}$\n",
        "\n",
        "$\\small {^1}\\text{nav.naveenvenkat@gmail.com}, $\n",
        "$\\small {^2}\\text{gaurangs@iisc.ac.in} $\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOpq5vVbpm57",
        "colab_type": "text"
      },
      "source": [
        "## Topics covered here\n",
        "\n",
        "- Colab basics\n",
        "- Linear Regression (tensor operations)\n",
        "- NN Model (nn.Module)\n",
        "- Autograd (loss optimization)\n",
        "- Training strategy (data loading / transforms / training loop / LR Scheduling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KclNlRe0ym9",
        "colab_type": "text"
      },
      "source": [
        "## Quick Links\n",
        "\n",
        "- **Stefan Otte:** [Youtube](https://www.youtube.com/watch?v=_H3aw6wkCv0&t=1s) / [Github](https://github.com/sotte/pytorch_tutorial) (**Highly Recommended**)\n",
        "- PyTorch: [60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
        "- DLCV: [Slides](https://drive.google.com/file/d/1gHo0eGvSbfTAukrpVIzts4r-zHUX369s/view?ts=5e2665f8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qt6o1WAuj_x2"
      },
      "source": [
        "### More References\n",
        "([courtesy](https://github.com/sotte/pytorch_tutorial/blob/master/notebooks/foreword.ipynb))\n",
        "- Twitter: https://twitter.com/PyTorch\n",
        "- Forum: https://discuss.pytorch.org/\n",
        "- Tutorials: https://pytorch.org/tutorials/\n",
        "- Examples: https://github.com/pytorch/examples\n",
        "- API Reference: https://pytorch.org/docs/stable/index.html\n",
        "- Torchvision: https://pytorch.org/docs/stable/torchvision/index.html\n",
        "- PyTorch Text: https://github.com/pytorch/text\n",
        "- PyTorch Audio: https://github.com/pytorch/audio\n",
        "- AllenNLP: https://allennlp.org/\n",
        "- Object detection/segmentation: https://github.com/facebookresearch/maskrcnn-benchmark\n",
        "- FAIR Sequence-to-Sequence Toolkit (PyTorch): https://github.com/pytorch/fairseq\n",
        "- FastAI http://www.fast.ai/\n",
        "- Stanford CS230 Deep Learning notes https://cs230-stanford.github.io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B7CkUR7qCAs",
        "colab_type": "text"
      },
      "source": [
        "### Visualization Tools\n",
        "- [TensorboardX](https://tensorboardx.readthedocs.io/en/latest/tutorial.html)\n",
        "- [Matplotlib](https://matplotlib.org/) / [Seaborn](https://seaborn.pydata.org/examples/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88LL8Lfel-7b",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6be-ndfYg8H",
        "colab_type": "text"
      },
      "source": [
        "## Google Colab\n",
        "\n",
        "iPython notebook with a full fledged linux kernel\n",
        "\n",
        "Use '!' or '%' before a linux shell command\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b_tqbJJYteG",
        "colab_type": "code",
        "outputId": "8e6d29c7-d63a-4c23-adbb-c97de66eaa01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"iPython Notebook\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iPython Notebook\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsMbDdihrSb8",
        "colab_type": "text"
      },
      "source": [
        "**Free GPU Access.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TegcZ1fNq7Ey",
        "colab_type": "code",
        "outputId": "27ea7cbd-1146-415b-c4b9-95a0a7b2a32e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Feb 13 07:28:40 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNJIMVmqQFCC",
        "colab_type": "text"
      },
      "source": [
        "**Install New Packages.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63-b_gByaLbG",
        "colab_type": "code",
        "outputId": "460e3491-e9a3-4a37-8378-8ddb009395eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!sudo apt install cowsay\n",
        "!ln -s /usr/games/cowsay /usr/bin/cowsay\n",
        "!cowsay \"It's a full-fledged Linux Kernel.\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cowsay is already the newest version (3.03+dfsg2-4).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "ln: failed to create symbolic link '/usr/bin/cowsay': File exists\n",
            " ___________________________________\n",
            "< It's a full-fledged Linux Kernel. >\n",
            " -----------------------------------\n",
            "        \\   ^__^\n",
            "         \\  (oo)\\_______\n",
            "            (__)\\       )\\/\\\n",
            "                ||----w |\n",
            "                ||     ||\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wj-G4xRczpi",
        "colab_type": "text"
      },
      "source": [
        "**Navigate through the system.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_DGytWs3RyS",
        "colab_type": "code",
        "outputId": "4083d5dc-a1b2-42ac-e09d-ddcc197ad0e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "%ls .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mbin\u001b[0m/                                       \u001b[01;34metc\u001b[0m/    \u001b[01;34mopt\u001b[0m/    \u001b[01;34msys\u001b[0m/\n",
            "\u001b[01;34mboot\u001b[0m/                                      \u001b[01;34mhome\u001b[0m/   \u001b[01;34mproc\u001b[0m/   \u001b[01;34mtensorflow-2.1.0\u001b[0m/\n",
            "\u001b[01;34mcontent\u001b[0m/                                   \u001b[01;34mlib\u001b[0m/    \u001b[01;34mroot\u001b[0m/   \u001b[30;42mtmp\u001b[0m/\n",
            "\u001b[01;34mdatalab\u001b[0m/                                   \u001b[01;34mlib32\u001b[0m/  \u001b[01;34mrun\u001b[0m/    \u001b[01;34mtools\u001b[0m/\n",
            "\u001b[01;34mdev\u001b[0m/                                       \u001b[01;34mlib64\u001b[0m/  \u001b[01;34msbin\u001b[0m/   \u001b[01;34musr\u001b[0m/\n",
            "dlib-19.18.0-cp27-cp27mu-linux_x86_64.whl  \u001b[01;34mmedia\u001b[0m/  \u001b[01;34msrv\u001b[0m/    \u001b[01;34mvar\u001b[0m/\n",
            "dlib-19.18.0-cp36-cp36m-linux_x86_64.whl   \u001b[01;34mmnt\u001b[0m/    \u001b[01;34mswift\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBP5_iNjrRU3",
        "colab_type": "text"
      },
      "source": [
        "**Caveat.** Runtime resets after 12 hours of GPU compute. All contents in the RAM / Disk will be cleared.\n",
        "\n",
        "**Solution.** Use Google Drive to save model states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_YG8OW_ilUkx",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive_dlcv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ChkpLIX3mSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !ls /gdrive_dlcv/\"My Drive\"\n",
        "%mkdir /gdrive_dlcv/\"My Drive\"/dlcv_example\n",
        "%cd /gdrive_dlcv/\"My Drive\"/dlcv_example\n",
        "%pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuPi9BukxSUV",
        "colab_type": "text"
      },
      "source": [
        "**Note.** The sidebar on the left (top-left, Files button) provides a nice interface to browse folders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29bzxo4QXhZ3",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch Basics | [Preview](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKBictUCXnt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standard imports for pytorch\n",
        "import torch\n",
        "import torch.nn as nn               # Neural Network module\n",
        "import torch.nn.functional as F     # A functional interface"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OaMFwOxdMmS",
        "colab_type": "text"
      },
      "source": [
        "A straightforward way of using GPU is as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Qj9mzLu-UNx",
        "colab_type": "code",
        "outputId": "31283fcd-b738-42e8-d829-bb1b19e54b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.randn(1).cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5895], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWMFgnHQ-Z59",
        "colab_type": "code",
        "outputId": "e16e5cae-6197-4531-f67d-491b0a485e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Feb 13 07:31:13 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    33W / 250W |    717MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRjgB6FCXzwH",
        "colab_type": "text"
      },
      "source": [
        "### Tensors: The atoms of Machine Learning | [Documentation](https://pytorch.org/docs/stable/tensors.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFTYWquLbSX5",
        "colab_type": "text"
      },
      "source": [
        "Torch tensors are similar to numpy arrays. Think of them as wrappers around numpy arrays, with the ability to be pushed onto GPU memory, and having constructs to define computational graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afD2mmIcXzKQ",
        "colab_type": "code",
        "outputId": "98ed9e63-c29f-4ffa-e8e3-e0aa3d0f0267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "numpy_arr = np.random.rand(2, 3, 4)\n",
        "print('numpy_arr\\n',numpy_arr,'\\n')\n",
        "\n",
        "torch_tensor = torch.rand(2, 3, 4)\n",
        "print('torch_tensor\\n',torch_tensor,'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numpy_arr\n",
            " [[[0.95285474 0.05352695 0.29484587 0.12978637]\n",
            "  [0.57402209 0.27446974 0.37925616 0.54452879]\n",
            "  [0.26790079 0.17351803 0.67307255 0.2073661 ]]\n",
            "\n",
            " [[0.95897443 0.44900745 0.81289086 0.80945103]\n",
            "  [0.96837033 0.09778685 0.09971418 0.61259549]\n",
            "  [0.77772058 0.70256355 0.08744634 0.04652971]]] \n",
            "\n",
            "torch_tensor\n",
            " tensor([[[0.9059, 0.2722, 0.3162, 0.3966],\n",
            "         [0.4677, 0.5194, 0.0407, 0.4732],\n",
            "         [0.0407, 0.3761, 0.5306, 0.3476]],\n",
            "\n",
            "        [[0.5020, 0.9219, 0.4249, 0.7185],\n",
            "         [0.4118, 0.7014, 0.4604, 0.8753],\n",
            "         [0.5072, 0.4911, 0.9322, 0.9515]]]) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0SMJFpdahkr",
        "colab_type": "code",
        "outputId": "36950d25-cc8d-4a8b-8fcb-0778ba8533b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "digits = torch.FloatTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) # np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=np.float64)\n",
        "print(digits)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkng6wP-bEgG",
        "colab_type": "code",
        "outputId": "da658c3b-21f5-4348-f115-cc3141fa2aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "ones = torch.ones((3, 5)) # np.ones(shape=(3, 5), dtype=np.float64)\n",
        "print(ones)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvR1oIzdbdoT",
        "colab_type": "text"
      },
      "source": [
        "Basic operations: +, -, *, /, **, $sum()$, $prod()$ etc.\n",
        "\n",
        "(see below for more)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bep01H5PbD-e",
        "colab_type": "code",
        "outputId": "4431a542-6b84-4b4f-ee93-5d908e975e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# ones -> (3, 5)\n",
        "# ones.sum(dim=0) -> (1, 5) or (5) (reduce the dimension)\n",
        "\n",
        "print(ones.sum(dim=0)) # numpy_arr.sum()\n",
        "print(torch.sum(ones, dim=0)) # np.sum(arr, axis=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eWcSq0RcMe-",
        "colab_type": "text"
      },
      "source": [
        "**Cloning** and **In-place** operations (note the last underscore)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB7zSUJncIk5",
        "colab_type": "code",
        "outputId": "c743ebe3-753f-4694-a34b-e3b36c36cc49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "print('ones')\n",
        "print(ones)\n",
        "\n",
        "ones_clone = ones.clone() # numpy_arr.copy()\n",
        "ones_clone.zero_() # Also achieved by ones_clone *= 0.\n",
        "\n",
        "'''\n",
        "Note: zero_() will be useful when gradients of previous backprop have to be \n",
        "cleared for the next backprop (see below)\n",
        "'''\n",
        "\n",
        "print('\\nones_clone - set to zero')\n",
        "print(ones_clone)\n",
        "\n",
        "ones_clone_2 = ones.clone()\n",
        "ones_clone_2.add_(12)\n",
        "\n",
        "print('\\nones_clone_2 - add 12 in place')\n",
        "print(ones_clone_2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ones\n",
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.]])\n",
            "\n",
            "ones_clone - set to zero\n",
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "\n",
            "ones_clone_2 - add 12 in place\n",
            "tensor([[13., 13., 13., 13., 13.],\n",
            "        [13., 13., 13., 13., 13.],\n",
            "        [13., 13., 13., 13., 13.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvD5h_3Zc6ha",
        "colab_type": "text"
      },
      "source": [
        "**Indexing** operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUDQrphOdha4",
        "colab_type": "code",
        "outputId": "fa203732-880e-4877-bb04-9bb1e9350dd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print('\\n', 'digits is', digits)\n",
        "print('\\n', 'digits[3:6] is', digits[3:6])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " digits is tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
            "\n",
            " digits[3:6] is tensor([3., 4., 5.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHlOoZWMfP9c",
        "colab_type": "text"
      },
      "source": [
        "**Viewing** a tensor with a different shape (Defaults to row-major order)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CTvDdilfN4B",
        "colab_type": "code",
        "outputId": "344656d1-d20c-4a4f-86a2-4898ce44148f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "digits_2d = digits.view(2,5) # np_arr.reshape(2,5)\n",
        "print(digits_2d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 1., 2., 3., 4.],\n",
            "        [5., 6., 7., 8., 9.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYbIwNxSriRK",
        "colab_type": "text"
      },
      "source": [
        "Transpose & Matrix Multiply"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PeT0qeVrkbP",
        "colab_type": "code",
        "outputId": "fca5ceef-86f6-47e2-972b-7b1329bf0e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "digits_square = torch.FloatTensor([1,2,3,4,5,6,7,8,9]).view(3,3)\n",
        "print('\\ndigits_square\\n',digits_square)\n",
        "\n",
        "digits_square_t = digits_square.t()\n",
        "print('\\ndigits_square_t\\n',digits_square_t)\n",
        "\n",
        "matrix_mul = digits_square_t @ digits_square\n",
        "print('\\nmatrix_mul\\n',matrix_mul)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "digits_square\n",
            " tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.],\n",
            "        [7., 8., 9.]])\n",
            "\n",
            "digits_square_t\n",
            " tensor([[1., 4., 7.],\n",
            "        [2., 5., 8.],\n",
            "        [3., 6., 9.]])\n",
            "\n",
            "matrix_mul\n",
            " tensor([[ 66.,  78.,  90.],\n",
            "        [ 78.,  93., 108.],\n",
            "        [ 90., 108., 126.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i38Vn-BnfAlh",
        "colab_type": "text"
      },
      "source": [
        "Broadcasting operations (repeating operations along extra dimensions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSKRoQjGbg8E",
        "colab_type": "code",
        "outputId": "a529d792-71c0-48ec-dc89-c1efb6592933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "print('--- broadcast scalar operations ---')\n",
        "\n",
        "fives = torch.ones((3, 5)) * 5\n",
        "print('\\n',fives)\n",
        "\n",
        "threes = torch.ones((3, 5)) + 2\n",
        "print('\\n',threes)\n",
        "\n",
        "print('\\n', '--- raise to the power ---')\n",
        "print('\\n', threes ** 4)\n",
        "\n",
        "print('\\n', '--- logarithm ---')\n",
        "print('\\n', torch.log(threes))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- broadcast scalar operations ---\n",
            "\n",
            " tensor([[5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.]])\n",
            "\n",
            " tensor([[3., 3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3., 3.]])\n",
            "\n",
            " --- raise to the power ---\n",
            "\n",
            " tensor([[81., 81., 81., 81., 81.],\n",
            "        [81., 81., 81., 81., 81.],\n",
            "        [81., 81., 81., 81., 81.]])\n",
            "\n",
            " --- logarithm ---\n",
            "\n",
            " tensor([[1.0986, 1.0986, 1.0986, 1.0986, 1.0986],\n",
            "        [1.0986, 1.0986, 1.0986, 1.0986, 1.0986],\n",
            "        [1.0986, 1.0986, 1.0986, 1.0986, 1.0986]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IggTHJFcdJ_b",
        "colab_type": "code",
        "outputId": "5e712333-ab21-44b6-d58d-48290164c34c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "print('\\n', '--- element-wise add/subtract ---')\n",
        "print('\\n', fives - threes)\n",
        "\n",
        "print('\\n', '--- element-wise multiply/divide ---')\n",
        "print('\\n', fives * threes)\n",
        "\n",
        "print('\\n', '--- mean / std ---')\n",
        "print('\\n', digits.mean(), digits.std())\n",
        "\n",
        "print('\\n', '--- mean along a given dimension ---')\n",
        "print('\\n', threes.mean(dim=1))\n",
        "\n",
        "print('\\n', '--- minimum / maximum / argmin / argmax ---')\n",
        "print('\\n', digits_2d)\n",
        "print('\\n', digits_2d.max(dim=0)[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " --- element-wise add/subtract ---\n",
            "\n",
            " tensor([[2., 2., 2., 2., 2.],\n",
            "        [2., 2., 2., 2., 2.],\n",
            "        [2., 2., 2., 2., 2.]])\n",
            "\n",
            " --- element-wise multiply/divide ---\n",
            "\n",
            " tensor([[15., 15., 15., 15., 15.],\n",
            "        [15., 15., 15., 15., 15.],\n",
            "        [15., 15., 15., 15., 15.]])\n",
            "\n",
            " --- mean / std ---\n",
            "\n",
            " tensor(4.5000) tensor(3.0277)\n",
            "\n",
            " --- mean along a given dimension ---\n",
            "\n",
            " tensor([3., 3., 3.])\n",
            "\n",
            " --- minimum / maximum / argmin / argmax ---\n",
            "\n",
            " tensor([[0., 1., 2., 3., 4.],\n",
            "        [5., 6., 7., 8., 9.]])\n",
            "\n",
            " tensor([5., 6., 7., 8., 9.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJS1DrNkep3p",
        "colab_type": "code",
        "outputId": "82f40984-955b-42e9-c5fa-8aef82127c87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print('\\n', '--- broadcast multiplication along the dimension with size 1 ---')\n",
        "\n",
        "fives_vector = fives[0, :]\n",
        "print('\\n', 'A:\\n', fives_vector, 'with shape:', fives_vector.shape) # Shape = (5)\n",
        "\n",
        "print('\\n', 'A.unsqueeze(0):\\n', fives_vector.unsqueeze(0), 'with shape:', fives_vector.unsqueeze(0).shape)\n",
        "\n",
        "print('\\n', 'B:\\n', threes, 'with shape:', threes.shape) # Shape = (3, 5)\n",
        "\n",
        "print('\\n', 'A * B:\\n', fives_vector.unsqueeze(0) * threes, 'with shape:', (fives_vector.unsqueeze(0) * threes).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " --- broadcast multiplication along the dimension with size 1 ---\n",
            "\n",
            " A:\n",
            " tensor([5., 5., 5., 5., 5.]) with shape: torch.Size([5])\n",
            "\n",
            " A.unsqueeze(0):\n",
            " tensor([[5., 5., 5., 5., 5.]]) with shape: torch.Size([1, 5])\n",
            "\n",
            " B:\n",
            " tensor([[3., 3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3., 3.]]) with shape: torch.Size([3, 5])\n",
            "\n",
            " A * B:\n",
            " tensor([[15., 15., 15., 15., 15.],\n",
            "        [15., 15., 15., 15., 15.],\n",
            "        [15., 15., 15., 15., 15.]]) with shape: torch.Size([3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-VcZ3lagRxF",
        "colab_type": "text"
      },
      "source": [
        "**Obtaining CPU <-> GPU** \n",
        "\n",
        "Note: it makes a clone of the tensor - it will copy the computational graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNj4UZuiYOYZ",
        "colab_type": "code",
        "outputId": "6834786b-f96f-4198-fae3-acca76dcae6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "fives_gpu = fives.cuda() # fives.set_device('cuda:0')\n",
        "fives_cpu = fives.cpu()\n",
        "\n",
        "print('fives is on \\t', fives.device)\n",
        "print('fives_gpu is on ', fives_gpu.device)\n",
        "print('fives_cpu is on ', fives_cpu.device)\n",
        "\n",
        "# To switch to GPU by default, add this line after imports\n",
        "# torch.cuda.set_device('cuda:0')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fives is on \t cpu\n",
            "fives_gpu is on  cuda:0\n",
            "fives_cpu is on  cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1WhwAnGinoF",
        "colab_type": "text"
      },
      "source": [
        "**Obtaining numpy <-> torch**\n",
        "\n",
        "Note: the numpy version will not retain the computational graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6PIiipSiulG",
        "colab_type": "code",
        "outputId": "daac82ec-7ab7-4352-f130-864eb77afeaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "fives_numpy = fives.numpy()\n",
        "print('fives_numpy is \\t\\t\\t', type(fives_numpy))\n",
        "\n",
        "fives_tensor_from_numpy = torch.from_numpy(fives_numpy)\n",
        "print('fives_tensor_from_numpy is \\t', type(fives_tensor_from_numpy))\n",
        "print()\n",
        "\n",
        "# Remember to convert to CPU before calling .numpy()\n",
        "# fives_gpu_numpy = fives_gpu.numpy() # Throws error\n",
        "# Best way will be as follows\n",
        "# fives_numpy = fives_gpu.cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fives_numpy is \t\t\t <class 'numpy.ndarray'>\n",
            "fives_tensor_from_numpy is \t <class 'torch.Tensor'>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7TEet_Hg2nv",
        "colab_type": "text"
      },
      "source": [
        "**The Variable class** \n",
        "- In short, Variable was meant to be a wrapper over tensors.\n",
        "- `variable.data` would contain the tensor \n",
        "- `variable.grad` would be used to store the gradient corresponding to the variable.\n",
        "\n",
        "Deprecated after torch v0.4. Tensors behave exactly like variables.\n",
        "\n",
        "(stable PyTorch version at the time of writing this notebook: v1.4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19AV6y7YjMOb",
        "colab_type": "code",
        "outputId": "6364ff89-7531-424d-d2e0-504962b0d128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "var = torch.autograd.Variable(fives, requires_grad=True)\n",
        "print(var)\n",
        "print('\\nVariable returns a Tensor object')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.]], requires_grad=True)\n",
            "\n",
            "Variable returns a Tensor object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUb5uCsqkGFY",
        "colab_type": "text"
      },
      "source": [
        "## AutoGrad Basics: Automatic Differentiation Package | [Reference](https://pytorch.org/docs/stable/autograd.html)\n",
        "\n",
        "- Tensorflow: Define your graph and then you run it\n",
        "- PyTorch: Create your graph, by running it\n",
        "    - View outputs live.\n",
        "    - Change forward prop / backprop / losses on-the-fly\n",
        "    - Use debugger (set_trace)\n",
        "- Run forward-prop as regular numpy operations. Backprop is taken care by AutoGrad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8zQ6TcXlt73",
        "colab_type": "text"
      },
      "source": [
        "**Linear regression example.** Consider the line: $ y = 3x + 5. $\n",
        "\n",
        "We will learn this line using linear regression. We start with the line $ f(x, \\{m,c\\}) = mx + c $. \n",
        "\n",
        "Forward prop using data as usual. \n",
        "\n",
        "Calculate the loss $L = (y - f(x))^2$ and backpropagate.\n",
        "\n",
        "During backprop, Autograd calculates $\\frac{\\delta L}{\\delta m}$ and $\\frac{\\delta L}{\\delta c}$ and stores it in `m.grad` and `c.grad`. We access these values to update the parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3oS_PzYk7Nk",
        "colab_type": "text"
      },
      "source": [
        "### Dataset Prep."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8h2EiYCkM8f",
        "colab_type": "code",
        "outputId": "2373c36f-4bce-4965-8351-83e5d984c165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def ground_truth(x):\n",
        "    return 3*x + 5\n",
        "\n",
        "def get_data(n_samples):\n",
        "    x = torch.rand((n_samples,))\n",
        "    y = ground_truth(x)\n",
        "    return (x,y)\n",
        "\n",
        "X, Y = get_data(1000)\n",
        "print('X:', X[:5], '\\nY:', Y[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: tensor([0.3085, 0.8632, 0.4199, 0.0014, 0.1167]) \n",
            "Y: tensor([5.9256, 7.5896, 6.2597, 5.0043, 5.3502])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzgGAaIRk9R_",
        "colab_type": "text"
      },
      "source": [
        "### Learnable Parameters / function\n",
        "Each tensor is associated with a flag `requires_grad` which signals Autograd to calculate gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xfFJ6WDuc_F",
        "colab_type": "code",
        "outputId": "17a6a57a-d1d8-459b-daf2-c747a203d879",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "random_tensor = torch.rand(1,)\n",
        "print(random_tensor.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eBghDgnvo4J",
        "colab_type": "text"
      },
      "source": [
        "Note: Set `requires_grad=True` for any tensor that requires a gradient (eg., $\\frac{\\delta L}{\\delta m}$ is the gradent corresponding to $m$ which is stored in `m.grad`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AbNzfvnk_i-",
        "colab_type": "code",
        "outputId": "941e5b87-82bf-4342-9127-6156b6a8e781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "m = torch.rand((1,), requires_grad=True) # Set the flag while creating the tensor\n",
        "c = torch.rand((1,))\n",
        "c.requires_grad_(True) # Or even after creating it using in-place operation\n",
        "\n",
        "print('m:', m)\n",
        "print('c:', c)\n",
        "\n",
        "def f(x, m, c):\n",
        "    # f -> m*x + c\n",
        "    scale = m * x\n",
        "    shift = scale + c\n",
        "    # print('\\nscale.grad_fn:',scale.grad_fn) # Un-comment these lines to see the grad_fn\n",
        "    # print('\\nshift.grad_fn:',shift.grad_fn) # Un-comment these lines to see the grad_fn\n",
        "    return shift"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "m: tensor([0.1936], requires_grad=True)\n",
            "c: tensor([0.3985], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKECiau9vQIk",
        "colab_type": "code",
        "outputId": "ea9e204c-4063-4657-e964-7c18a04d8008",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "out = f(0.5, m, c)\n",
        "print('\\nout:', out)\n",
        "\n",
        "out.backward()\n",
        "\n",
        "print('\\nm.grad:', m.grad)\n",
        "print('\\nc.grad:', c.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "out: tensor([0.4953], grad_fn=<AddBackward0>)\n",
            "\n",
            "m.grad: tensor([0.5000])\n",
            "\n",
            "c.grad: tensor([1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yopV_DugwO_-",
        "colab_type": "text"
      },
      "source": [
        "**Important Note:** Run the cell above multiple times and see the effect.\n",
        "\n",
        "Gradients accumulate (add over the previous gradients) by default. Need to clear gradients before the next backward call"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9c9yQ-FwZPa",
        "colab_type": "code",
        "outputId": "a074d440-a8f2-4c92-e9c4-1bdc312618a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "out = f(0.5, m, c)\n",
        "print('out:', out)\n",
        "\n",
        "m.grad.zero_()\n",
        "c.grad.zero_()\n",
        "out.backward()\n",
        "\n",
        "print('m.grad:', m.grad)\n",
        "print('c.grad:', c.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "out: tensor([0.4953], grad_fn=<AddBackward0>)\n",
            "m.grad: tensor([0.5000])\n",
            "c.grad: tensor([1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vVuXzlelqEh",
        "colab_type": "text"
      },
      "source": [
        "### Create the Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TpIkT1mVstb0",
        "outputId": "95b5d485-2218-4552-f358-242b89718e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "from IPython.core.debugger import set_trace # Use this to set trace during forward pass / backward pass etc.\n",
        "\n",
        "# Get data\n",
        "X, Y = get_data(1000)\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(X, Y)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16)\n",
        "\n",
        "# Create Learnable Function\n",
        "m = torch.rand((1,), requires_grad=True)\n",
        "c = torch.rand((1,), requires_grad=True)\n",
        "\n",
        "def f(inp, m, c):\n",
        "    return inp * m + c\n",
        "\n",
        "# Set Flags / Settings\n",
        "max_epochs = 50\n",
        "lr = 0.01\n",
        "\n",
        "# Training / Validation Statistics\n",
        "loss_values = []\n",
        "\n",
        "# Training Loop\n",
        "for e in range(max_epochs):\n",
        "\n",
        "    for dat in dataloader:\n",
        "\n",
        "        x, y = dat\n",
        "\n",
        "        if m.grad: m.grad.zero_()\n",
        "        if c.grad: c.grad.zero_()\n",
        "\n",
        "        pred = f(x, m, c)\n",
        "        \n",
        "        loss = ((y-pred)**2).mean()\n",
        "\n",
        "        loss.backward() # Calculate grad(loss, m) and grad(loss, c)\n",
        "\n",
        "        m.data = m.data - lr * m.grad.data\n",
        "        c.data = c.data - lr * c.grad.data\n",
        "\n",
        "        loss_values.append(loss.cpu().detach().numpy())\n",
        "    \n",
        "    if e%5 == 0:\n",
        "        print('epoch', e)\n",
        "        print('new params')\n",
        "        print('m:',m.data)\n",
        "        print('c:',c.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "new params\n",
            "m: tensor([2.3844])\n",
            "c: tensor([4.2817])\n",
            "epoch 5\n",
            "new params\n",
            "m: tensor([2.8841])\n",
            "c: tensor([5.0628])\n",
            "epoch 10\n",
            "new params\n",
            "m: tensor([2.9226])\n",
            "c: tensor([5.0421])\n",
            "epoch 15\n",
            "new params\n",
            "m: tensor([2.9482])\n",
            "c: tensor([5.0282])\n",
            "epoch 20\n",
            "new params\n",
            "m: tensor([2.9653])\n",
            "c: tensor([5.0189])\n",
            "epoch 25\n",
            "new params\n",
            "m: tensor([2.9768])\n",
            "c: tensor([5.0126])\n",
            "epoch 30\n",
            "new params\n",
            "m: tensor([2.9845])\n",
            "c: tensor([5.0084])\n",
            "epoch 35\n",
            "new params\n",
            "m: tensor([2.9896])\n",
            "c: tensor([5.0057])\n",
            "epoch 40\n",
            "new params\n",
            "m: tensor([2.9931])\n",
            "c: tensor([5.0038])\n",
            "epoch 45\n",
            "new params\n",
            "m: tensor([2.9954])\n",
            "c: tensor([5.0025])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N0pM7aRyjI9",
        "colab_type": "code",
        "outputId": "1d72c7af-b742-4023-da2a-9faae69da0e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_values = np.array(loss_values)\n",
        "plt.plot(loss_values)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAATlUlEQVR4nO3dfZBddX3H8ff3bhIgwEBi1jTyYMLD\n6MRaA24jDo6CVgTqDFgdhY420zIT28qMzugfqNMWZ2x9GIWOHcc2FGrqWJ8QB6ZglSIOQ6fGLhAg\nISKBhkoMZHmOgIRkv/3jnrv37mY3u9ndu+f+2Pdr5nLP/d2ze75nz82Hc3/n4ReZiSSpPI26C5Ak\nTY8BLkmFMsAlqVAGuCQVygCXpEItmMuFLVu2LFeuXDmXi5Sk4t1xxx2PZ2b/2PY5DfCVK1cyODg4\nl4uUpOJFxMPjtduFIkmFMsAlqVAGuCQVygCXpEIZ4JJUKANckgplgEtSoYoI8N17fsuPtz5adxmS\n1FOKCPA/vmoT679xB3v3DdddiiT1jCIC/OEnngMgcfAJSWopIsAlSQeaNMAj4vCI+HlE3B0RWyPi\nM1X7qojYFBHbI+I7EbGoW0UGAYCjv0lS21T2wF8E3p6ZbwDWAOdGxBnAF4ArM/MU4Cngkm4VadeJ\nJB1o0gDPpt9ULxdWjwTeDlxbtW8ELuxKhaNq6fYSJKkcU+oDj4i+iNgM7AZuBh4Ens7MfdUsjwDH\nTfCz6yNiMCIGh4aGZlSse+KS1DalAM/M/Zm5BjgeWAu8dqoLyMwNmTmQmQP9/Qfcj3yKv2P0syTp\nEM9CycyngVuBNwPHRkRrQIjjgZ2zXNuBy+/2AiSpIFM5C6U/Io6tpo8A3glsoxnk76tmWwdc360i\nW9JdcEkaMZUh1VYAGyOij2bgfzcz/z0i7gO+HRGfBe4Cru5WkTnmWZI0hQDPzHuA08Zpf4hmf/ic\ncQdcktqKuBIzqme7UCSprYgAbzG/JamtiAC3D1ySDlREgLfYhSJJbWUFeN0FSFIPKSLAW3ve7oBL\nUlsRAd7ivVAkqa2oADe/JamtiAD/87edDJjfktSpiAA/YeliAIbtBJekEUUEePtKzFrLkKSeUkaA\nVwlufktSWxkBPjKosREuSS1FBHirD8X8lqS2IgI8Jp9FkuadMgI8Wl0oNRciST2kjACvnr0SU5La\nigjwRlWle+CS1FZEgLfOQvFCHklqKyPAPQ9ckg5QRIC3uAMuSW1FBHjrLBT3wSWprYwAr57dA5ek\ntkkDPCJOiIhbI+K+iNgaER+t2i+PiJ0Rsbl6nN+tIu0Dl6QDLZjCPPuAj2fmnRFxNHBHRNxcvXdl\nZn6pe+U1te+F0u0lSVI5Jg3wzNwF7Kqm90TENuC4bhfWqb0HboJLUssh9YFHxErgNGBT1XRpRNwT\nEddExJIJfmZ9RAxGxODQ0ND0ivRmVpJ0gCkHeEQcBXwf+FhmPgt8DTgZWENzD/3L4/1cZm7IzIHM\nHOjv759mmV7II0ljTSnAI2IhzfD+ZmZeB5CZj2Xm/swcBq4C1naryLt+9RQA1925s1uLkKTiTOUs\nlACuBrZl5hUd7Ss6ZnsPsGX2y2vatmsPAIMPP9WtRUhScaZyFsqZwIeAeyNic9X2KeDiiFhD8+y+\nHcCHu1IhMDzc7Drp88bgkjRiKmeh3M74YyrcNPvljG9/K8AbJrgktRRxJeb5r/8dAN606hU1VyJJ\nvaOIAD/zlGUAnPLKo2quRJJ6RxEB3uo6aXWlSJIKCfBGdSnmfs8Dl6QRZQR4o3UvFANcklqKCPC+\n1h74cM2FSFIPKSLAW4Ma24UiSW1FBHhrD9wuFElqKyLARw5iehaKJI0oI8A9jVCSDlBEgPc1HJFH\nksYqIsBbt0DxIKYktRUS4HahSNJYRQR4nxfySNIBigjwhhfySNIBCgnw5rN94JLUVkSARwSNsAtF\nkjoVEeDQ7EbxIKYktZUT4I2wC0WSOhQT4H0RXsgjSR2KCfBGeB64JHUqJ8Ab9oFLUqdiAryvEZ6F\nIkkdJg3wiDghIm6NiPsiYmtEfLRqXxoRN0fEA9Xzkq4WGh7ElKROU9kD3wd8PDNXA2cAH4mI1cBl\nwC2ZeSpwS/W6a5qnEXZzCZJUlkkDPDN3Zead1fQeYBtwHHABsLGabSNwYbeKBOhreCGPJHU6pD7w\niFgJnAZsApZn5q7qrUeB5RP8zPqIGIyIwaGhoekX6oU8kjTKlAM8Io4Cvg98LDOf7Xwvm7vG46Zr\nZm7IzIHMHOjv759+ofaBS9IoUwrwiFhIM7y/mZnXVc2PRcSK6v0VwO7ulNjUPAulm0uQpLJM5SyU\nAK4GtmXmFR1v3QCsq6bXAdfPfnltXsgjSaMtmMI8ZwIfAu6NiM1V26eAzwPfjYhLgIeB93enxCbv\nhSJJo00a4Jl5OxATvP2O2S1nYs17oRjgktRSzJWYnoUiSaOVE+ANL+SRpE7FBLgX8kjSaMUEuOeB\nS9JoZQW4feCSNKKYAPdCHkkarZgA90IeSRqtoAC3D1ySOhUT4H2NYNg9cEkaUUyANyIYdg9ckkaU\nE+CNYL/5LUkjignwvsAuFEnqUE6AN+xCkaROxQR4eCGPJI1STID3eRBTkkYpJ8AbgTvgktRWTICH\nBzElaZRiArzPIdUkaZRyAtw+cEkapZgAjwiGHZFHkkYUE+B9De9GKEmdCgpwu1AkqVMxAR72gUvS\nKJMGeERcExG7I2JLR9vlEbEzIjZXj/O7W2bzIKZdKJLUNpU98K8D547TfmVmrqkeN81uWQfyQh5J\nGm3SAM/M24An56CWg/JCHkkabSZ94JdGxD1VF8uSiWaKiPURMRgRg0NDQ9NeWF8Ee17cx65nXpj2\n75Ckl5PpBvjXgJOBNcAu4MsTzZiZGzJzIDMH+vv7p7k4eG7vPgA++M+bpv07JOnlZFoBnpmPZeb+\nzBwGrgLWzm5ZB3r2t80A373nxW4vSpKKMK0Aj4gVHS/fA2yZaN7ZcuM9uwDYUwW5JM13CyabISK+\nBZwFLIuIR4C/Ac6KiDVAAjuAD3exRknSOCYN8My8eJzmq7tQiyTpEBRzJaYkaTQDXJIKZYBLUqEM\ncEkqlAEuSYUywCWpUMUE+OJFfXWXIEk9pZgA/6t3r667BEnqKcUEeF8j6i5BknpKMQEuSRqtmAB/\nYe/+ukuQpJ5STIC/943HA3DO6uU1VyJJvaGYAD/qsAW86pjDOeaIhXWXIkk9oZgAB4gI9qfjYkoS\nFBbgjQaY35LUVFSA90UwbIJLElBYgDciGDa/JQkoLMAjYNgElySgsAB/cOg5brx3V91lSFJPKCrA\nJUltBrgkFarIAN9vP7gklRnge/cN112CJNVu0gCPiGsiYndEbOloWxoRN0fEA9Xzku6WOZpXY0rS\n1PbAvw6cO6btMuCWzDwVuKV6PWf27zfAJWnSAM/M24AnxzRfAGyspjcCF85yXQflHrgkTb8PfHlm\ntk7IfhSY8B6vEbE+IgYjYnBoaGiaixvNg5iSNAsHMTMzgQkTNTM3ZOZAZg709/fPdHGAAS5JMP0A\nfywiVgBUz7tnr6TJ2YUiSdMP8BuAddX0OuD62SlnarwfiiRN7TTCbwH/DbwmIh6JiEuAzwPvjIgH\ngD+oXnfdlR94A2AXiiQBLJhshsy8eIK33jHLtUyqEQHAPgNcksq6ErOv0QxwB3WQpNICvNoDtwtF\nkkoL8IYBLkktBrgkFaqoAG+0Atw+cEkqK8BbfeCeBy5JhQX4gr5mgL/k3QglqawAX9TXLPel/Q7o\nIEllBfgCA1ySWooK8IXugUvSiCID/EXHxJSksgK83QfuQUxJKivA7QOXpBFFBfjCkdMIDXBJKivA\nqz3wvfaBS1JZAd7qA//sjdtqrkSS6ldUgLfOQpEkFRbgrbsRSpIKC3BJUluxAb7rmRfqLkGSalVs\ngP/2Jc9EkTS/FRvg9oZLmu+KDfB9DuogaZ6bUYBHxI6IuDciNkfE4GwVdTDr3vxqAPYN24UiaX6b\njT3wszNzTWYOzMLvmtSZpywDYJ83tJI0zxXXhdIaVs2R6SXNdzMN8AR+HBF3RMT68WaIiPURMRgR\ng0NDQzNcHPQ1miXbBy5pvptpgL8lM08HzgM+EhFvHTtDZm7IzIHMHOjv75/h4mBBdTWmN7SSNN/N\nKMAzc2f1vBv4AbB2Noo6mKE9LwLwxR/9otuLkqSeNu0Aj4gjI+Lo1jRwDrBltgqbyJPP7QXgrv97\nutuLkqSetmAGP7sc+EFEtH7Pv2Xmf8xKVQfRGtRBkua7ae+BZ+ZDmfmG6vG6zPzb2SxsIu994/HN\n59OPn4vFSVLPKu40wsWLFrCgEbzw0r66S5GkWhUX4NA8hfCmex+tuwxJqlWRAX7aicfWXYIk1a7I\nAD/7Na8EHJ1e0vxWZIAvXtQHwPN799dciSTVp8gAb12N+czzL9VciSTVp8gAv/bORwD4p9serLkS\nSapPkQF+6dmnArB21dKaK5Gk+hQZ4K9+xWIANv/Ky+klzV9FBvgRC5sHMf/lv3bUW4gk1ajIAF+5\n7EgAPnjGiTVXIkn1KTLAW25/4PG6S5Ck2hQd4DueeL7uEiSpNkUHOLTvDy5J803xAf69wV/VXYIk\n1aLYAP/shb8LwOd+6NBqkuanYgN8yeJFdZcgSbUqNsDPed3yken9w1ljJZJUj2IDfGFfu/Stv36m\nxkokqR7FBjjA2a/pB+Anv9hdcyWSNPeKDvC//8BpANyw+dc1VyJJc6/oAD9m8UJOXLqYhx5/jk0P\nPVF3OZI0p4oOcIBXHXs4AB/Y8DOHWJM0r8wowCPi3Ii4PyK2R8Rls1XUobji/WtGpt/9ldsZ9owU\nSfPEtAM8IvqArwLnAauBiyNi9WwVNlWvOvYIbvn42wC4/7E9nPSpm1j1yRvZsvMZhoeTTANd0svT\nghn87Fpge2Y+BBAR3wYuAO6bjcIOxcn9R/HTT5zFWV/6KQCZ8O5/uB2ARQsaLFm8kAWNBo0GNCKI\nZr0E0PzP1B3i7HMmolcrkw7dy/HT/Hd/9Hp+f+XsjiI2kwA/Dui8EckjwJvGzhQR64H1ACee2L37\nd69cdiQ7Pv+HDO15kc/9cBunvvJonn5+L089v5cg2FftjQ9nMpyQMOneeTL6g5QTvVE3v2ToZSRf\nph/o1kA0s2kmAT4lmbkB2AAwMDDQ9S3Tf/Rho/rFJenlaiYHMXcCJ3S8Pr5qkyTNgZkE+P8Ap0bE\nqohYBFwE3DA7ZUmSJjPtLpTM3BcRlwI/AvqAazJz66xVJkk6qBn1gWfmTcBNs1SLJOkQFH8lpiTN\nVwa4JBXKAJekQhngklSomMt7hUTEEPDwNH98GfD4LJYz10qvH8pfB+uvX+nrUFf9r87M/rGNcxrg\nMxERg5k5UHcd01V6/VD+Olh//Upfh16r3y4USSqUAS5JhSopwDfUXcAMlV4/lL8O1l+/0tehp+ov\npg9ckjRaSXvgkqQOBrgkFaqIAO+FwZOnIiJ2RMS9EbE5IgartqURcXNEPFA9L6naIyK+Uq3TPRFx\neg31XhMRuyNiS0fbIdcbEeuq+R+IiHU9sA6XR8TOajtsjojzO977ZLUO90fEuzraa/mMRcQJEXFr\nRNwXEVsj4qNVexHb4SD1F7ENIuLwiPh5RNxd1f+Zqn1VRGyqavlOdctsIuKw6vX26v2Vk61XV2Vm\nTz9o3qr2QeAkYBFwN7C67romqHUHsGxM2xeBy6rpy4AvVNPnAz+kOTjbGcCmGup9K3A6sGW69QJL\ngYeq5yXV9JKa1+Fy4BPjzLu6+vwcBqyqPld9dX7GgBXA6dX00cAvqzqL2A4Hqb+IbVD9HY+qphcC\nm6q/63eBi6r2fwT+opr+S+Afq+mLgO8cbL26XX8Je+Ajgydn5l6gNXhyKS4ANlbTG4ELO9r/NZt+\nBhwbESvmsrDMvA14ckzzodb7LuDmzHwyM58CbgbO7X71TROsw0QuAL6dmS9m5v8C22l+vmr7jGXm\nrsy8s5reA2yjOd5sEdvhIPVPpKe2QfV3/E31cmH1SODtwLVV+9i/f2u7XAu8IyKCiderq0oI8PEG\nTz7YB6ROCfw4Iu6I5mDOAMszc1c1/SiwvJru1fU61Hp7dT0urboYrml1P9Dj61B9HT+N5l5gcdth\nTP1QyDaIiL6I2Azspvk/vgeBpzNz3zi1jNRZvf8M8Apqqr+EAC/JWzLzdOA84CMR8dbON7P5XauY\n8zZLq7fD14CTgTXALuDL9ZYzuYg4Cvg+8LHMfLbzvRK2wzj1F7MNMnN/Zq6hOa7vWuC1NZc0ZSUE\neDGDJ2fmzup5N/ADmh+Gx1pdI9Xz7mr2Xl2vQ62359YjMx+r/lEOA1fR/irbk+sQEQtpht83M/O6\nqrmY7TBe/aVtA4DMfBq4FXgzza6p1ohlnbWM1Fm9fwzwBDXVX0KAFzF4ckQcGRFHt6aBc4AtNGtt\nnRGwDri+mr4B+JPqrIIzgGc6vjLX6VDr/RFwTkQsqb4mn1O11WbMsYT30NwO0FyHi6ozCVYBpwI/\np8bPWNV/ejWwLTOv6HiriO0wUf2lbIOI6I+IY6vpI4B30uzHvxV4XzXb2L9/a7u8D/hJ9Q1povXq\nrm4fJZ2NB80j77+k2Tf16brrmaDGk2gehb4b2Nqqk2b/2C3AA8B/AkuzffT7q9U63QsM1FDzt2h+\nvX2JZp/dJdOpF/gzmgdttgN/2gPr8I2qxnto/sNa0TH/p6t1uB84r+7PGPAWmt0j9wCbq8f5pWyH\ng9RfxDYAfg+4q6pzC/DXVftJNAN4O/A94LCq/fDq9fbq/ZMmW69uPryUXpIKVUIXiiRpHAa4JBXK\nAJekQhngklQoA1ySCmWAS1KhDHBJKtT/A9B7mc/BzpB6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfsKAT32x_Vn",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network Modules and Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cui8qMZlRSi5",
        "colab_type": "text"
      },
      "source": [
        "We can define the learnable parameters as objects of `nn.Parameter()`. These will be listed in `Line.parameters()` and `Line.state_dict()`. This is useful when you want to get a list of all the parameters in a module to, say, save the module state during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvQ3J6p1y5vt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Line(nn.Module): # Recommended way \n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(Line, self).__init__()\n",
        "\n",
        "        self.m = torch.nn.Parameter((torch.rand(1,))) # Define parameters (these will be listed in Line.parameters() and Line.state_dict())\n",
        "        self.c = torch.nn.Parameter((torch.rand(1,)))\n",
        "        self.dummy_variable = torch.rand((3,5))\n",
        "        # self.dummy_var_parameterized = torch.nn.Parameter(self.dummy_variable)\n",
        "\n",
        "    def forward(self, x): # Default forward function. Invoked when f(x) is Called (see below).\n",
        "        return self.m * x + self.c\n",
        "\n",
        "    def forward2(self, x, k='scale_and_shift'): # Custom forward functions\n",
        "        if k=='scale_and_shift':\n",
        "            return self.m * x + self.c\n",
        "        elif k=='scale_only':\n",
        "            return self.m * x\n",
        "        elif k=='shift_only':\n",
        "            return self.x + self.c\n",
        "        elif k=='identity':\n",
        "            return self.x\n",
        "        else:\n",
        "            raise NotImplementedError('Not implemented forward2 for k=\"{}\"'.format(str(k)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo6JV9YVNWmp",
        "colab_type": "text"
      },
      "source": [
        "Not only does the Line module encapsulate all the necessary variables, the `nn.Module` API is pretty versatile, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RLfNWldM2Tn",
        "colab_type": "code",
        "outputId": "74cdbdb0-b5d6-460d-cab5-ff068600849c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "f = Line()\n",
        "params = list( f.parameters() )\n",
        "\n",
        "# Default forward pass - calling the object invokes forward() function.\n",
        "# print(f(0.4))\n",
        "\n",
        "# Defining and calling custom forward functions\n",
        "# f.forward2(x, 'F2')\n",
        "\n",
        "# Custom forward-pass by accessing the elements of the object\n",
        "# result = f.m * (0.5) + f.c\n",
        "\n",
        "for i in params:\n",
        "    print('\\n',i)\n",
        "\n",
        "print('\\n--- state dictionary ---')\n",
        "print(f.state_dict())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Parameter containing:\n",
            "tensor([0.2779], requires_grad=True)\n",
            "\n",
            " Parameter containing:\n",
            "tensor([0.6440], requires_grad=True)\n",
            "\n",
            "--- state dictionary ---\n",
            "OrderedDict([('m', tensor([0.2779])), ('c', tensor([0.6440]))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIqLR06USUa1",
        "colab_type": "text"
      },
      "source": [
        "The size of the training loop significantly reduces using `nn.Module`. Now we can create optimizers which take all trainable parameters and perform the gradient descent `step` at once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNdy5oVS0hSN",
        "colab_type": "code",
        "outputId": "34c3f214-07de-42de-f97e-76691ce22be8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X, Y = get_data(1000)\n",
        "dataset = torch.utils.data.TensorDataset(X, Y)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16)\n",
        "\n",
        "f = Line()\n",
        "optimizer = torch.optim.SGD(f.parameters(), lr=0.001) # Pass f's parameters to the optimizer\n",
        "\n",
        "for e in range(max_epochs):\n",
        "\n",
        "    for dat in dataloader:\n",
        "\n",
        "        x, y = dat\n",
        "\n",
        "        pred = f(x)\n",
        "        \n",
        "        loss = ((y-pred)**2).mean()\n",
        "\n",
        "        # if m.grad: m.grad.data.zero_()\n",
        "        # if c.grad: c.grad.data.zero_()\n",
        "        optimizer.zero_grad() # This zeros out the gradients of every parameter that is passed to the optimizer\n",
        "\n",
        "        loss.backward(create_graph=True) # Calculate grad(loss, m) and grad(loss, c)\n",
        "\n",
        "        # m.data = m.data - lr * m.grad.data\n",
        "        # c.data = c.data - lr * c.grad.data\n",
        "        optimizer.step() # This performs the gradient descent step (including momentum calculations etc.)\n",
        "\n",
        "    \n",
        "    if e%5 == 0:\n",
        "        print('\\nepoch', e)\n",
        "        print('new params')\n",
        "        print('m:',f.m)\n",
        "        print('c:',f.c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 0\n",
            "new params\n",
            "m: Parameter containing:\n",
            "tensor([0.8956], requires_grad=True)\n",
            "c: Parameter containing:\n",
            "tensor([1.1062], requires_grad=True)\n",
            "\n",
            "epoch 5\n",
            "new params\n",
            "m: Parameter containing:\n",
            "tensor([2.0661], requires_grad=True)\n",
            "c: Parameter containing:\n",
            "tensor([3.2572], requires_grad=True)\n",
            "\n",
            "epoch 10\n",
            "new params\n",
            "m: Parameter containing:\n",
            "tensor([2.5888], requires_grad=True)\n",
            "c: Parameter containing:\n",
            "tensor([4.2182], requires_grad=True)\n",
            "\n",
            "epoch 15\n",
            "new params\n",
            "m: Parameter containing:\n",
            "tensor([2.8220], requires_grad=True)\n",
            "c: Parameter containing:\n",
            "tensor([4.6477], requires_grad=True)\n",
            "\n",
            "epoch 20\n",
            "new params\n",
            "m: Parameter containing:\n",
            "tensor([2.9260], requires_grad=True)\n",
            "c: Parameter containing:\n",
            "tensor([4.8396], requires_grad=True)\n",
            "\n",
            "epoch 25\n",
            "new params\n",
            "m: Parameter containing:\n",
            "tensor([2.9722], requires_grad=True)\n",
            "c: Parameter containing:\n",
            "tensor([4.9255], requires_grad=True)\n",
            "\n",
            "epoch 30\n",
            "new params\n",
            "m: Parameter containing:\n",
            "tensor([2.9927], requires_grad=True)\n",
            "c: Parameter containing:\n",
            "tensor([4.9640], requires_grad=True)\n",
            "\n",
            "epoch 35\n",
            "new params\n",
            "m: Parameter containing:\n",
            "tensor([3.0016], requires_grad=True)\n",
            "c: Parameter containing:\n",
            "tensor([4.9813], requires_grad=True)\n",
            "\n",
            "epoch 40\n",
            "new params\n",
            "m: Parameter containing:\n",
            "tensor([3.0054], requires_grad=True)\n",
            "c: Parameter containing:\n",
            "tensor([4.9891], requires_grad=True)\n",
            "\n",
            "epoch 45\n",
            "new params\n",
            "m: Parameter containing:\n",
            "tensor([3.0069], requires_grad=True)\n",
            "c: Parameter containing:\n",
            "tensor([4.9927], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViQkki0VgIwv",
        "colab_type": "text"
      },
      "source": [
        "## Train and Eval Mode - BatchNorm example\n",
        "\n",
        "Recall that BatchNorm for 1D input calculates the following\n",
        "\n",
        "$$ \\frac{(X - \\hat{\\mu}_i)}{\\hat{\\sigma_i}} \\cdot \\gamma + \\beta $$\n",
        "\n",
        "where, $\\gamma$ and $\\beta$ are the learnable scale and shift parameters, while $\\hat{\\mu}$ and $\\hat{\\sigma}$ are the running batch statistics calculated for iteration $i$ as:\n",
        "\n",
        "$$ \\hat{\\mu}_i = \\alpha \\cdot \\hat{\\mu}_{i-1} + (1-\\alpha) \\cdot \\mu_i $$\n",
        "$$ \\hat{\\sigma}_i = \\alpha \\cdot \\hat{\\sigma}_{i-1} + (1-\\alpha) \\cdot \\sigma_i $$\n",
        "\n",
        "and $\\mu$ and $\\sigma$ are the mean and variance of the batch ($X$). Note that because of the need to calculate running batch statistics the Training and the Testing phases of BatchNorm are different. During Training, the running statistics are updated while at Test time, the statistics are frozen. Thus, we need different modes of execution during Training and Testing phases. This can be achieved using flags inside the module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faENJB5qgODU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myBatchNorm1d(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, input_dims, alpha=0.9, track_running_status=True):\n",
        "\n",
        "        super(myBatchNorm1d, self).__init__()\n",
        "\n",
        "        # Create running mean and std as trainable parameters\n",
        "        self.running_mean = nn.Parameter( torch.zeros((input_dims,)), requires_grad=False ) # Note, requires_grad is false\n",
        "        self.running_std = nn.Parameter( torch.ones((input_dims,)), requires_grad=False ) # Note, requires_grad is false\n",
        "\n",
        "        # alpha is a constant weight used for the exponential time-average.\n",
        "        self.alpha = nn.Parameter( torch.FloatTensor([alpha]), requires_grad=False ) # Note, requires_grad is false, but this is defined as a parameter (we want to save this value along with the model)\n",
        "\n",
        "        # track_running_status is a flag which is used to determine the mode of execution (training / testing). We define it as a parameter because we want \n",
        "        # it to be listed in Module.state_dict().\n",
        "        self.track_running_status = nn.Parameter(torch.ones(1, dtype=torch.bool) * track_running_status, requires_grad=False)\n",
        "\n",
        "        # Scale and Shift are the trainable parameters (requires_grad=True).\n",
        "        self.scale = nn.Parameter( (torch.rand((input_dims,))), requires_grad=True ) # Note, requires_grad is false\n",
        "        self.shift = nn.Parameter( (torch.rand((input_dims,))), requires_grad=True ) # Note, requires_grad is false\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if self.track_running_status:\n",
        "\n",
        "            m = x.mean(dim=0)\n",
        "            s = x.std(dim=0)\n",
        "            alpha = self.alpha\n",
        "            self.running_mean.data = self.running_mean.data * (alpha) + m.detach() * (1-alpha) # Detach m and s to be used as constants\n",
        "            self.running_std.data = self.running_std.data * (alpha) + s.detach() * (1-alpha)\n",
        "\n",
        "            normalized = (x - self.running_mean) / torch.sqrt(  (self.running_std**2) + 1e-10  ) # Adding epsilon (1e-10) for numerical stability.\n",
        "\n",
        "            output = normalized * self.scale + self.shift\n",
        "        \n",
        "        else:\n",
        "\n",
        "            normalized = (x - self.running_mean) / torch.sqrt(  (self.running_std**2) + 1e-10  )\n",
        "            output = normalized * self.scale + self.shift\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        self.track_running_status = nn.Parameter(torch.ones(1, dtype=torch.bool) * True, requires_grad=False)\n",
        "\n",
        "\n",
        "    def eval(self):\n",
        "        self.track_running_status = nn.Parameter(torch.ones(1, dtype=torch.bool) * False, requires_grad=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdf2UuZugj-g",
        "colab_type": "code",
        "outputId": "ee17bd0a-55ff-4490-b829-c020c61ab041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "bn_module = myBatchNorm1d(input_dims=5)\n",
        "# print(list(bn_module.parameters()))\n",
        "print(bn_module.state_dict())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('running_mean', tensor([0., 0., 0., 0., 0.])), ('running_std', tensor([1., 1., 1., 1., 1.])), ('alpha', tensor([0.9000])), ('track_running_status', tensor([True])), ('scale', tensor([0.9347, 0.7057, 0.2425, 0.0082, 0.5353])), ('shift', tensor([0.0707, 0.7157, 0.3098, 0.3806, 0.3252]))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWT3UospeE5B",
        "colab_type": "text"
      },
      "source": [
        "In train mode, the running statistics update. The following cell does multiple forward passes with BatchNorm in Train mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkge9nTgguXn",
        "colab_type": "code",
        "outputId": "2c66fba5-37df-434d-9bb9-d92c44a7d060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "bn_module.train()\n",
        "\n",
        "for i in range(3):\n",
        "    r = torch.rand((64,5))\n",
        "    bn_module(r)\n",
        "    print('\\nrunning_mean:', bn_module.state_dict()['running_mean'])\n",
        "    print('running_std:', bn_module.state_dict()['running_std'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "running_mean: tensor([0.0635, 0.0458, 0.0531, 0.0461, 0.0448])\n",
            "running_std: tensor([0.9257, 0.9298, 0.9295, 0.9282, 0.9288])\n",
            "\n",
            "running_mean: tensor([0.1059, 0.0854, 0.0987, 0.0925, 0.0894])\n",
            "running_std: tensor([0.8594, 0.8661, 0.8645, 0.8607, 0.8667])\n",
            "\n",
            "running_mean: tensor([0.1426, 0.1256, 0.1343, 0.1310, 0.1334])\n",
            "running_std: tensor([0.8041, 0.8132, 0.8081, 0.8033, 0.8109])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ZnEJ0He_Yh",
        "colab_type": "text"
      },
      "source": [
        "In `eval` mode, the running statistics are not updated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY-WCGiYeDNc",
        "colab_type": "code",
        "outputId": "1c5a565d-77fd-4f8c-b314-dcb533f5d806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "bn_module.eval()\n",
        "\n",
        "for i in range(3):\n",
        "    r = torch.rand((64,5))\n",
        "    bn_module(r)\n",
        "    print('\\nrunning_mean:', bn_module.state_dict()['running_mean'])\n",
        "    print('running_std:', bn_module.state_dict()['running_std'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "running_mean: tensor([0.1426, 0.1256, 0.1343, 0.1310, 0.1334])\n",
            "running_std: tensor([0.8041, 0.8132, 0.8081, 0.8033, 0.8109])\n",
            "\n",
            "running_mean: tensor([0.1426, 0.1256, 0.1343, 0.1310, 0.1334])\n",
            "running_std: tensor([0.8041, 0.8132, 0.8081, 0.8033, 0.8109])\n",
            "\n",
            "running_mean: tensor([0.1426, 0.1256, 0.1343, 0.1310, 0.1334])\n",
            "running_std: tensor([0.8041, 0.8132, 0.8081, 0.8033, 0.8109])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdWEZA2cWQLn",
        "colab_type": "text"
      },
      "source": [
        "### A note on using `.detach`\n",
        "The `detach` function returns a copy of the reference to the tensor while removing all information about the gradients and the computational graph (see below, `grad` and `grad_fn` yield `None`). This is useful when you want to treat activations as constants. For eg. we might want to use the output of a module as a weight for the loss function, but may not want backpropagation to happen through the weight. In such a situation, `detach` can be called to obtain a copy of the activation as a constant.\n",
        "\n",
        "This can also be seen in the BatchNorm module above. Lines 31-32 in class myBatchNorm1d use `m.detach()` and `c.detach()` which ensures that the computational graph is not carried forward while updating the running statistics.\n",
        "\n",
        "An example showing the effect of `detach` is as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "05eddd70-7f32-4eed-d597-7cf315e4039d",
        "id": "x9lgVHIOWITJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "y = torch.rand(1, requires_grad = True)\n",
        "k = y**2\n",
        "k.backward()\n",
        "\n",
        "print(y)\n",
        "print(y.grad)\n",
        "print(y.detach().grad)\n",
        "print(y.detach().grad_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.2295], requires_grad=True)\n",
            "tensor([0.4589])\n",
            "None\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIHUdDgyf_Ka",
        "colab_type": "text"
      },
      "source": [
        "**Similarity between `.data` and `.detach`.** Modifying the value of `.detach` or `.data` will modify the contents of the tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11rr_GaYgdel",
        "colab_type": "code",
        "outputId": "ff3b71c7-d1d5-4288-ad1a-6ee434037e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# For detach\n",
        "y = torch.rand(1, requires_grad = True)\n",
        "k = y**2\n",
        "k.backward()\n",
        "print('y:', y)\n",
        "print('y.grad:', y.grad)\n",
        "\n",
        "y_detach = y.detach()\n",
        "print('\\ny_detach:', y_detach)\n",
        "print('y_detach.grad:', y_detach.grad)\n",
        "\n",
        "print('\\n--- after modifying the contents of y_detach ---')\n",
        "\n",
        "y_detach.zero_() # Modify the contents of y_detach\n",
        "print('\\ny_detach:', y_detach)\n",
        "print('y:', y) # Contents of y change"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y: tensor([0.9689], requires_grad=True)\n",
            "y.grad: tensor([1.9377])\n",
            "\n",
            "y_detach: tensor([0.9689])\n",
            "y_detach.grad: None\n",
            "\n",
            "--- after modifying the contents of y_detach ---\n",
            "\n",
            "y_detach: tensor([0.])\n",
            "y: tensor([0.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfJ2aRMciVMX",
        "colab_type": "code",
        "outputId": "98802b3c-3ac2-4bb3-81d1-a3a8e2b2c1c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# For data\n",
        "y = torch.rand(1, requires_grad = True)\n",
        "k = y**2\n",
        "k.backward()\n",
        "print('y:', y)\n",
        "print('y.grad:', y.grad)\n",
        "\n",
        "y_data = y.data\n",
        "print('\\ny_data:', y_data)\n",
        "print('y_data.grad:', y_data.grad)\n",
        "\n",
        "print('\\n--- after modifying the contents of y_data ---')\n",
        "\n",
        "y_data.zero_() # Modify the contents of y_data\n",
        "print('\\ny_data:', y_data)\n",
        "print('y:', y) # Contents of y change"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y: tensor([0.4178], requires_grad=True)\n",
            "y.grad: tensor([0.8356])\n",
            "\n",
            "y_data: tensor([0.4178])\n",
            "y_data.grad: None\n",
            "\n",
            "--- after modifying the contents of y_data ---\n",
            "\n",
            "y_data: tensor([0.])\n",
            "y: tensor([0.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0FK_Ar5iiDZ",
        "colab_type": "text"
      },
      "source": [
        "**Difference between `.data` and `.detach`.** If you use `.detach` instead of `.data`, and modify the value of the tensor, PyTorch will throw a Runtime Error during gradient computation involving the tensor that was modified [Forum thread](https://github.com/pytorch/pytorch/issues/6990#issuecomment-384680164). This is useful when you want to prevent errors caused by in-place operations. \n",
        "\n",
        "However, modifying `.data` does not throw this error.\n",
        "\n",
        "Here is an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsVnGaKRjaCm",
        "colab_type": "code",
        "outputId": "4311c706-8e02-471e-ccba-5bc7a6a5a845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "# Detach\n",
        "y = torch.rand(1, requires_grad = True)\n",
        "out = torch.exp(y)\n",
        "\n",
        "out.backward(retain_graph=True)\n",
        "print(y.grad)\n",
        "\n",
        "out_detach = out.detach()\n",
        "print(out_detach.zero_().add_(10))\n",
        "\n",
        "out.backward() # Throws error suggesting that the value of out (required for gradient computation) has been modified by an in-place operation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.8414])\n",
            "tensor([10.])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-e99068f68fec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_detach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Throws error suggesting that the value of out (required for gradient computation) has been modified by an in-place operation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1]], which is output 0 of ExpBackward, is at version 2; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZsPLQ4JloAX",
        "colab_type": "code",
        "outputId": "e94d22a4-f538-443a-8447-9b2ce3128636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Data\n",
        "y = torch.rand(1, requires_grad = True)\n",
        "out = torch.exp(y)\n",
        "\n",
        "out.backward(retain_graph=True)\n",
        "print(y.grad)\n",
        "\n",
        "out_data = out.data\n",
        "print(out_data.zero_().add_(10))\n",
        "\n",
        "out.backward() # Calculates the gradients \n",
        "\n",
        "print(y.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.0097])\n",
            "tensor([10.])\n",
            "tensor([11.0097])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9AHXKRM1i6w",
        "colab_type": "text"
      },
      "source": [
        "## MNIST Demo\n",
        "\n",
        "Here, we will combine all that we have learned till now and train a simple MNIST classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eaX22UA2mFE",
        "colab_type": "text"
      },
      "source": [
        "### Load data\n",
        "\n",
        "- [List of transforms](https://pytorch.org/docs/stable/torchvision/transforms.html) (Key transforms: Normalize, RandomRotatation, RandomFlip, RandomCrop, ToTensor)\n",
        "- [Other torchvision datasets](https://pytorch.org/docs/stable/torchvision)\n",
        "\n",
        "Note: ToTensor() converts (0,255) to (0,1) range, and converts `B * H * W * C` to `B * C * H * W`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpUTxd2i2nTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "\n",
        "transforms = transforms.Compose([\n",
        "                                #  transforms.RandomRotation(degrees=5),\n",
        "                                #  transforms.RandomCrop(size=28, padding=4),\n",
        "                                 transforms.ToTensor()\n",
        "                                 ])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_dataset = MNIST(root='./mnist_data', download=True, transform=transforms, train=True) # Can use the gdrive path as well: /gdrive_dlcv/My Drive/dlcv/mnist_data\n",
        "test_dataset = MNIST(root='./mnist_data', download=True, transform=transforms, train=False)\n",
        "mnist_train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "mnist_test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCREC5DpzBvm",
        "colab_type": "text"
      },
      "source": [
        "### Create the model\n",
        "We will use the Modules defined in `torch.nn` to build a CNN. Note that each layer defined below is a child of `nn.Module`. See the official [Documentation](https://pytorch.org/docs/stable/nn.html) for an exhaustive list of Layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FOcOrYqyBmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet_Plus(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LeNet_Plus, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,32,kernel_size=5, dilation=1, stride=1, padding=2,bias=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32,64,kernel_size=5,dilation=1, stride=1, padding=2,bias=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1   = nn.Linear(64*7*7,1024)\n",
        "        self.fc2   = nn.Linear(1024, 10)\n",
        "\n",
        "        # The following dictionary will be useful for directly accessing layers / modules\n",
        "        self.components = {\n",
        "            'bb': self.conv2,\n",
        "            'fc': self.fc1,\n",
        "            'fc2': self.fc2\n",
        "        }\n",
        "        \n",
        "    def forward(self, input):\n",
        "\n",
        "        # conv1 + max pool\n",
        "        out = F.relu(self.conv1(input))\n",
        "        out = self.pool1(out)\n",
        "        \n",
        "        # conv2 + max pool\n",
        "        out = F.relu(self.conv2(out))\n",
        "        out = self.pool2(out)\n",
        "        \n",
        "        # fc-1\n",
        "        B,C,H,W = out.size()\n",
        "        out = out.view(B,-1) \n",
        "        out = F.relu(self.fc1(out))\n",
        "        \n",
        "        # Logits\n",
        "        out = self.fc2(out) # Note: Softmax is not applied. We will manually apply during loss calculation.\n",
        "        \n",
        "        return out\n",
        "\n",
        "model = LeNet_Plus()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q5052BQZCm_",
        "colab_type": "text"
      },
      "source": [
        "We can use the following functions to save and load models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyWquyRwT5r2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model_state_dict(model, filename):\n",
        "    torch.save(model.state_dict(), filename)\n",
        "\n",
        "def load_model_state_dict(model, filename):\n",
        "    sd = torch.load(filename)\n",
        "    model.load_state_dict(sd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSGjOeKqzFwu",
        "colab_type": "text"
      },
      "source": [
        "### Execute the train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCc2S7Pr2GX7",
        "colab_type": "code",
        "outputId": "e8d3d65d-7109-43fd-b45c-30f7a4048683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = LeNet_Plus().cuda() # Note: Cuda!\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "max_epochs = 20\n",
        "max_accuracy = 0.\n",
        "\n",
        "load_model = False\n",
        "\n",
        "if load_model == True:\n",
        "    msd = torch.load('best_model')\n",
        "    model.load_state_dict(msd)\n",
        "    osd = torch.load('best_optimizer')\n",
        "    optimizer.load_state_dict(osd)\n",
        "\n",
        "for e in range(1, max_epochs+1):\n",
        "\n",
        "    # SET THE MODEL TO TRAIN\n",
        "    model.train()\n",
        "\n",
        "    for dat in mnist_train_dataloader:\n",
        "\n",
        "        x, y = dat\n",
        "        n = x.shape[0]\n",
        "\n",
        "        x = x.cuda() # Note: Cuda!\n",
        "        y = y.cuda()\n",
        "\n",
        "        pred = model(x)\n",
        "\n",
        "        ''' Method-1 : Manually get the loss '''\n",
        "        # pred_s = F.softmax(pred, dim=-1) # broadcast softmax operation over entire matrix, taking softmax along dim=-1 (last dimension)\n",
        "        # loss_matrix = - torch.log(pred_s)\n",
        "        # total_loss = 0\n",
        "        # for i in range(n):\n",
        "        #     total_loss += loss_matrix[i, y[i]]\n",
        "        # loss = total_loss / n\n",
        "\n",
        "        ''' Method-2 : Convert y into a One-hot vector and broadcast '''\n",
        "        # pred_s = F.softmax(pred, dim=-1)\n",
        "        # loss_matrix = - torch.log(pred_s)\n",
        "        # y_one_hot = F.one_hot(y, num_classes=10) # B x 10\n",
        "        # total_loss = (y_one_hot * loss_matrix).sum(dim=-1).sum()\n",
        "        # loss = total_loss / n\n",
        "\n",
        "        ''' Method-3 : nn / functional inbuilt losses '''\n",
        "        loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
        "        total_loss = loss_fn(pred, y) # Note, softmax is calculated internally\n",
        "        loss = total_loss / n\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    # SET THE MODEL TO EVAL\n",
        "    model.eval()\n",
        "    \n",
        "    total_correct = 0.\n",
        "    total_count = 0.\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for dat in mnist_test_dataloader:\n",
        "\n",
        "            x, y = dat\n",
        "\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "\n",
        "            p = model(x)\n",
        "            highest_act, pred_label = torch.max(p, dim=-1)\n",
        "\n",
        "            total_correct += (pred_label == y).sum()\n",
        "            total_count += x.shape[0]\n",
        "\n",
        "    print('\\nepoch', e)\n",
        "    acc = total_correct / total_count\n",
        "    print('Test accuracy:', acc)\n",
        "\n",
        "    # Condition to save the model / optimizer state\n",
        "    if acc > max_accuracy:\n",
        "        max_accuracy = acc\n",
        "        torch.save(model.state_dict(), 'best_model')\n",
        "        torch.save(optimizer.state_dict(), 'best_optimizer')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1\n",
            "Test accuracy: tensor(0.8844, device='cuda:0')\n",
            "\n",
            "epoch 2\n",
            "Test accuracy: tensor(0.9208, device='cuda:0')\n",
            "\n",
            "epoch 3\n",
            "Test accuracy: tensor(0.9445, device='cuda:0')\n",
            "\n",
            "epoch 4\n",
            "Test accuracy: tensor(0.9543, device='cuda:0')\n",
            "\n",
            "epoch 5\n",
            "Test accuracy: tensor(0.9647, device='cuda:0')\n",
            "\n",
            "epoch 6\n",
            "Test accuracy: tensor(0.9694, device='cuda:0')\n",
            "\n",
            "epoch 7\n",
            "Test accuracy: tensor(0.9706, device='cuda:0')\n",
            "\n",
            "epoch 8\n",
            "Test accuracy: tensor(0.9741, device='cuda:0')\n",
            "\n",
            "epoch 9\n",
            "Test accuracy: tensor(0.9771, device='cuda:0')\n",
            "\n",
            "epoch 10\n",
            "Test accuracy: tensor(0.9781, device='cuda:0')\n",
            "\n",
            "epoch 11\n",
            "Test accuracy: tensor(0.9804, device='cuda:0')\n",
            "\n",
            "epoch 12\n",
            "Test accuracy: tensor(0.9819, device='cuda:0')\n",
            "\n",
            "epoch 13\n",
            "Test accuracy: tensor(0.9822, device='cuda:0')\n",
            "\n",
            "epoch 14\n",
            "Test accuracy: tensor(0.9827, device='cuda:0')\n",
            "\n",
            "epoch 15\n",
            "Test accuracy: tensor(0.9843, device='cuda:0')\n",
            "\n",
            "epoch 16\n",
            "Test accuracy: tensor(0.9847, device='cuda:0')\n",
            "\n",
            "epoch 17\n",
            "Test accuracy: tensor(0.9842, device='cuda:0')\n",
            "\n",
            "epoch 18\n",
            "Test accuracy: tensor(0.9847, device='cuda:0')\n",
            "\n",
            "epoch 19\n",
            "Test accuracy: tensor(0.9857, device='cuda:0')\n",
            "\n",
            "epoch 20\n",
            "Test accuracy: tensor(0.9856, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FozoBvJt13hs",
        "colab_type": "text"
      },
      "source": [
        "## CIFAR10 Demo - Torchvision Models | Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWNLiwUrHD1a",
        "colab_type": "text"
      },
      "source": [
        "### Load the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8Um5EXZHFz3",
        "colab_type": "code",
        "outputId": "ef97c974-615f-4474-cabd-5ddc88a999ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "\n",
        "train_transforms = transforms.Compose([ \n",
        "                                 # Here, we add some augmentations as well for the training data\n",
        "                                 transforms.RandomRotation(degrees=5),\n",
        "                                 transforms.RandomCrop(size=32, padding=4),\n",
        "                                 transforms.ToTensor()\n",
        "                                 ])\n",
        "\n",
        "test_transforms = transforms.Compose([ \n",
        "                                # For test images, we do not add augmentations\n",
        "                                #  transforms.RandomRotation(degrees=5),\n",
        "                                #  transforms.RandomCrop(size=32, padding=4),\n",
        "                                 transforms.ToTensor()\n",
        "                                 ])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = CIFAR10(root='./cifar_data', download=True, transform=train_transforms, train=True)\n",
        "test_dataset = CIFAR10(root='./cifar_data', download=True, transform=test_transforms, train=False)\n",
        "cifar_train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "cifar_test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vayOcIcHOsP",
        "colab_type": "text"
      },
      "source": [
        "### Get the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmRxqfsc16V0",
        "colab_type": "code",
        "outputId": "41153353-b499-4068-ce73-0aee4564ef3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "81944e45b6424f9a8dc0bfba703584ea",
            "0495658e14a047b0931a573a863cd89c",
            "5f88d6751c47422f926d35108ad654b1",
            "76143aff8cd64c318903177fef9eea85",
            "53c36d21e6524b46a77d486528452a0f",
            "2f7ba9894fda4341857d3bbedb77e330",
            "207701d378e34dfe857f90e513b8d6e0",
            "7e3d2d503ea2423f9f30f249789d537d"
          ]
        }
      },
      "source": [
        "from torchvision.models import resnet18\n",
        "\n",
        "model = resnet18(pretrained=True) # Initialized from ImageNet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81944e45b6424f9a8dc0bfba703584ea",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=46827520), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RFcClJXn193",
        "colab_type": "text"
      },
      "source": [
        "We will use the backbone network as ResNet18 (upto the last layer). `model.children()` returns a generator to enumerate all submodules (objects of `nn.Module`) in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hln92dEwFmcV",
        "colab_type": "code",
        "outputId": "7bd1b410-ddf1-4bd3-da23-5b7c2f0e240b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(model)\n",
        "\n",
        "children = list(model.children())\n",
        "print('\\nNumber of submodules:', len(children))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n",
            "\n",
            "Number of submodules: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIrxyoHapprG",
        "colab_type": "text"
      },
      "source": [
        "Now we create a custom model using the resnet18 backbone, with FC layers at the end. \n",
        "\n",
        "**Using Sequential.** `nn.Sequential` composes the given list of `Modules` into a single `Module`. We will use this to create a single module called `self.bb` which will have all layers of resnet18 except the last FC layer (see L13-15 below).\n",
        "\n",
        "**Using `.children()` / `.modules()`.** Here is a [discussion](https://discuss.pytorch.org/t/module-children-vs-module-modules/4551/8) on `model.children()` and `model.modules()`. \n",
        "\n",
        "Specifically, `model.modules()` also returns `model` as a `Module` (the `model` itself is a module), and recursively enumerates the modules contained within `model` ([reference](https://discuss.pytorch.org/t/module-children-vs-module-modules/4551/2)).\n",
        "\n",
        "However, `model.children()` does not recursively enumerate. It just lists the modules inside `model` ([reference](https://discuss.pytorch.org/t/module-children-vs-module-modules/4551/4))\n",
        "\n",
        "This is in contrast to `model.parameters()` which returns each `Parameter()` in the model ([forum](https://discuss.pytorch.org/t/list-child-parameters/19702/3))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWy1xOGOFum_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomModel(nn.Module):\n",
        "\n",
        "    def __init__(self, train_backbone=False, pretrained=False):\n",
        "        super(CustomModel, self).__init__()\n",
        "\n",
        "        self.train_backbone = train_backbone # We can optionally set flags to train backbone etc.\n",
        "\n",
        "        resnet = resnet18(pretrained=pretrained) # Initialized from ImageNet - tuned to CIFAR10. \n",
        "        # Note: the class ResNet18 (defined in torchvision.models) is a child of nn.Module\n",
        "        # Also, the variable resnet defined above will not show up in model.state_dict() as it is a local variable.\n",
        "        # Had it been self.resnet = resnet18(), it is an element of the CustomModel object, and will show up in model.state_dict()\n",
        "\n",
        "        list_of_layers = list(resnet.children())[:-1] # List every layer but the last fc layer\n",
        "\n",
        "        self.bb = nn.Sequential(*list_of_layers) \n",
        "        # Note the asterisk. nn.Sequential does not take a list as input, rather requires each Module to be passed as a separate argument.\n",
        "        \n",
        "        self.fc1 = nn.Linear(512, 10)\n",
        "\n",
        "        # We can add more layers\n",
        "        # self.fc1 = nn.Linear(512, 256)\n",
        "        # self.act1 = nn.ELU()\n",
        "        # self.fc2 = nn.Linear(256, 10) # etc.\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        if self.train_backbone:\n",
        "            bb = self.bb(x)\n",
        "        else:\n",
        "            with torch.no_grad(): # Speeds up computation\n",
        "                bb = self.bb(x)\n",
        "            bb = bb.detach() # Optional, if no_grad context is used\n",
        "        flat = bb.squeeze()\n",
        "        f1 = self.fc1(flat)\n",
        "\n",
        "        return f1\n",
        "    \n",
        "    def set_mode_train(self):\n",
        "\n",
        "        if self.train_backbone:\n",
        "            self.bb.train()\n",
        "            self.fc1.train()\n",
        "        \n",
        "        else:\n",
        "            self.bb.eval()\n",
        "            self.fc1.train()\n",
        "    \n",
        "    def set_mode_eval(self):\n",
        "\n",
        "        self.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8qUKMXwG4_V",
        "colab_type": "code",
        "outputId": "7709bf82-dee6-4b2f-e131-b920b78840da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "''' Train the entire model '''\n",
        "model = CustomModel(train_backbone=True, pretrained=False).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
        "\n",
        "''' Initialize from ImageNet and set a Low LR for backbone '''\n",
        "# model = CustomModel(train_backbone=True).cuda()\n",
        "# optimizer = torch.optim.SGD(\n",
        "#     [{'params':model.fc1.parameters(), 'lr':1e-2, 'momentum':0.9},\n",
        "#      {'params':model.bb.parameters(), 'lr':1e-4, 'momentum':0.9}]\n",
        "#     )\n",
        "\n",
        "''' Finetune the last layer '''\n",
        "# model = CustomModel(train_backbone=False).cuda()\n",
        "\n",
        "# # Method-1\n",
        "# optimizer = torch.optim.SGD(model.fc1.parameters(), lr=1e-2, momentum=0.9) # Pass the parameters of FC layer\n",
        "\n",
        "# # Method-2\n",
        "# # optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9) # This is fine here, since we handle the foward pass for the backbone using train_backbone flag)\n",
        "\n",
        "max_epochs = 50\n",
        "max_accuracy = 0.\n",
        "\n",
        "# Set load_model to true to continue training from the previous best state\n",
        "load_model = False\n",
        "\n",
        "if load_model == True:\n",
        "    msd = torch.load('best_model')\n",
        "    model.load_state_dict(msd)\n",
        "    osd = torch.load('best_optimizer')\n",
        "    optimizer.load_state_dict(osd)\n",
        "\n",
        "for e in range(1, max_epochs+1):\n",
        "\n",
        "    # SET THE MODEL TO TRAIN (Backbone is in eval as it contains batchnorm etc.)\n",
        "    model.set_mode_train()\n",
        "\n",
        "    for dat in cifar_train_dataloader:\n",
        "\n",
        "        x, y = dat\n",
        "        n = x.shape[0]\n",
        "\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        pred = model(x)\n",
        "\n",
        "        # Method-1 : Manually get the loss\n",
        "        # pred_s = F.softmax(pred, dim=-1) # broadcast softmax operation over entire matrix, taking softmax along dim=-1 (last dimension)\n",
        "        # loss_matrix = - torch.log(pred_s)\n",
        "        # total_loss = 0\n",
        "        # for i in range(n):\n",
        "        #     total_loss += loss_matrix[i, y[i]]\n",
        "        # loss = total_loss / n\n",
        "\n",
        "        # Method-2 : Convert y into a One-hot vector and broadcast\n",
        "        # pred_s = F.softmax(pred, dim=-1)\n",
        "        # loss_matrix = - torch.log(pred_s)\n",
        "        # y_one_hot = F.one_hot(y, num_classes=10)\n",
        "        # total_loss = (y_one_hot * loss_matrix).sum(dim=-1).sum()\n",
        "        # loss = total_loss / n\n",
        "\n",
        "        # Method-3 : nn / functional inbuilt losses\n",
        "        loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
        "        total_loss = loss_fn(pred, y) # Note, softmax is calculated internally\n",
        "        loss = total_loss / n\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        # break\n",
        "\n",
        "    # SET THE MODEL TO EVAL\n",
        "    model.set_mode_eval()\n",
        "    \n",
        "    total_correct = 0.\n",
        "    total_count = 0.\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for dat in cifar_test_dataloader: # In practice, use a validation set (not the test set!)\n",
        "\n",
        "            x, y = dat\n",
        "\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "\n",
        "            p = model(x)\n",
        "            highest_act, pred_label = torch.max(p, dim=-1)\n",
        "\n",
        "            total_correct += (pred_label == y).sum()\n",
        "            total_count += x.shape[0]\n",
        "            # break\n",
        "\n",
        "    print('\\nepoch', e)\n",
        "    acc = total_correct / total_count\n",
        "    print('Test accuracy:', acc)\n",
        "\n",
        "    if acc > max_accuracy:\n",
        "        max_accuracy = acc\n",
        "        torch.save(model.state_dict(), 'best_model')\n",
        "        torch.save(optimizer.state_dict(), 'best_optimizer')\n",
        "    \n",
        "    # Reducing learning rate every 5 epochs\n",
        "    if e % 5 == 0:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] /= 10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1\n",
            "Test accuracy: tensor(0.3658, device='cuda:0')\n",
            "\n",
            "epoch 2\n",
            "Test accuracy: tensor(0.3984, device='cuda:0')\n",
            "\n",
            "epoch 3\n",
            "Test accuracy: tensor(0.5030, device='cuda:0')\n",
            "\n",
            "epoch 4\n",
            "Test accuracy: tensor(0.5474, device='cuda:0')\n",
            "\n",
            "epoch 5\n",
            "Test accuracy: tensor(0.5882, device='cuda:0')\n",
            "\n",
            "epoch 6\n",
            "Test accuracy: tensor(0.6486, device='cuda:0')\n",
            "\n",
            "epoch 7\n",
            "Test accuracy: tensor(0.6546, device='cuda:0')\n",
            "\n",
            "epoch 8\n",
            "Test accuracy: tensor(0.6422, device='cuda:0')\n",
            "\n",
            "epoch 9\n",
            "Test accuracy: tensor(0.6638, device='cuda:0')\n",
            "\n",
            "epoch 10\n",
            "Test accuracy: tensor(0.6752, device='cuda:0')\n",
            "\n",
            "epoch 11\n",
            "Test accuracy: tensor(0.6836, device='cuda:0')\n",
            "\n",
            "epoch 12\n",
            "Test accuracy: tensor(0.6863, device='cuda:0')\n",
            "\n",
            "epoch 13\n",
            "Test accuracy: tensor(0.6893, device='cuda:0')\n",
            "\n",
            "epoch 14\n",
            "Test accuracy: tensor(0.6891, device='cuda:0')\n",
            "\n",
            "epoch 15\n",
            "Test accuracy: tensor(0.6938, device='cuda:0')\n",
            "\n",
            "epoch 16\n",
            "Test accuracy: tensor(0.6929, device='cuda:0')\n",
            "\n",
            "epoch 17\n",
            "Test accuracy: tensor(0.6889, device='cuda:0')\n",
            "\n",
            "epoch 18\n",
            "Test accuracy: tensor(0.6870, device='cuda:0')\n",
            "\n",
            "epoch 19\n",
            "Test accuracy: tensor(0.7002, device='cuda:0')\n",
            "\n",
            "epoch 20\n",
            "Test accuracy: tensor(0.6933, device='cuda:0')\n",
            "\n",
            "epoch 21\n",
            "Test accuracy: tensor(0.6877, device='cuda:0')\n",
            "\n",
            "epoch 22\n",
            "Test accuracy: tensor(0.6946, device='cuda:0')\n",
            "\n",
            "epoch 23\n",
            "Test accuracy: tensor(0.6970, device='cuda:0')\n",
            "\n",
            "epoch 24\n",
            "Test accuracy: tensor(0.6908, device='cuda:0')\n",
            "\n",
            "epoch 25\n",
            "Test accuracy: tensor(0.6982, device='cuda:0')\n",
            "\n",
            "epoch 26\n",
            "Test accuracy: tensor(0.6895, device='cuda:0')\n",
            "\n",
            "epoch 27\n",
            "Test accuracy: tensor(0.6918, device='cuda:0')\n",
            "\n",
            "epoch 28\n",
            "Test accuracy: tensor(0.6918, device='cuda:0')\n",
            "\n",
            "epoch 29\n",
            "Test accuracy: tensor(0.6956, device='cuda:0')\n",
            "\n",
            "epoch 30\n",
            "Test accuracy: tensor(0.6934, device='cuda:0')\n",
            "\n",
            "epoch 31\n",
            "Test accuracy: tensor(0.6889, device='cuda:0')\n",
            "\n",
            "epoch 32\n",
            "Test accuracy: tensor(0.6971, device='cuda:0')\n",
            "\n",
            "epoch 33\n",
            "Test accuracy: tensor(0.6944, device='cuda:0')\n",
            "\n",
            "epoch 34\n",
            "Test accuracy: tensor(0.6954, device='cuda:0')\n",
            "\n",
            "epoch 35\n",
            "Test accuracy: tensor(0.6991, device='cuda:0')\n",
            "\n",
            "epoch 36\n",
            "Test accuracy: tensor(0.6858, device='cuda:0')\n",
            "\n",
            "epoch 37\n",
            "Test accuracy: tensor(0.6964, device='cuda:0')\n",
            "\n",
            "epoch 38\n",
            "Test accuracy: tensor(0.6931, device='cuda:0')\n",
            "\n",
            "epoch 39\n",
            "Test accuracy: tensor(0.6888, device='cuda:0')\n",
            "\n",
            "epoch 40\n",
            "Test accuracy: tensor(0.6862, device='cuda:0')\n",
            "\n",
            "epoch 41\n",
            "Test accuracy: tensor(0.6952, device='cuda:0')\n",
            "\n",
            "epoch 42\n",
            "Test accuracy: tensor(0.7001, device='cuda:0')\n",
            "\n",
            "epoch 43\n",
            "Test accuracy: tensor(0.6937, device='cuda:0')\n",
            "\n",
            "epoch 44\n",
            "Test accuracy: tensor(0.6910, device='cuda:0')\n",
            "\n",
            "epoch 45\n",
            "Test accuracy: tensor(0.6957, device='cuda:0')\n",
            "\n",
            "epoch 46\n",
            "Test accuracy: tensor(0.6909, device='cuda:0')\n",
            "\n",
            "epoch 47\n",
            "Test accuracy: tensor(0.6957, device='cuda:0')\n",
            "\n",
            "epoch 48\n",
            "Test accuracy: tensor(0.6884, device='cuda:0')\n",
            "\n",
            "epoch 49\n",
            "Test accuracy: tensor(0.6812, device='cuda:0')\n",
            "\n",
            "epoch 50\n",
            "Test accuracy: tensor(0.6887, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx_YNG8-gqrm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Some helpful nn Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlAb3WDBm-7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myReLU(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(myReLU, self).__init__()\n",
        "        self.saved_activation = None\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        self.saved_activation = inp.detach() # We detach it, as we only need to refer to the magnitude of the activations\n",
        "        return inp.clamp(min=0)\n",
        "\n",
        "    def backward(self, inp_grad):\n",
        "        grad = inp_grad.clone()\n",
        "        grad[self.saved_activation < 0] = 0\n",
        "        return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNyATv1iebqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GradChange(nn.Module): # Useful for gradient reversal (eg. GAN training), gradient diminishing (eg. similar to having a low lr for backbone if using SGD without momentum)\n",
        "\n",
        "    def __init__(self, factor, requires_grad=False):\n",
        "        super(GradChange, self).__init__()\n",
        "        self.factor = nn.Parameter(data=factor, requires_grad=requires_grad)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "    def backward(self, x):\n",
        "        return self.factor * x"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}